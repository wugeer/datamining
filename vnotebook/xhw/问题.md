# 问题
###  yarn调度出现: ERROR executor.Executor: Exception in task 1.0 in stage 17.0 (TID 821) java.io.InvalidClassException: org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat; local class incompatible: stream classdesc serialVersionUID = -7145885503676937273, local class serialVersionUID = -102046493462843122

## 解决:权限问题
参见:[修改hdfs文件权限](http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/)
export HADOOP_USER_NAME ＝ sxcp 



ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 61.0 GB of 61 GB physical memory used

解决方法：移除RDD缓存操作，增加该JOB的spark.storage.memoryFraction系数值，增加该job的spark.yarn.executor.memoryOverhead值


在/etc/spark2/conf.cloudera.spark2_on_yarn/yarn-conf/yarn-site.xml 修改yarn.scheduler.maximum-allocation-mb值不起作用.